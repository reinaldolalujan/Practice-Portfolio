{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liputan6.id Dataset: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was carried out with @Roby Koeswojo, @Satriavi Dananjaya, and @Rijal Abdulhakim as part of Indonesia AI's Portfolio Project, with Text Summarization as the main topic.\n",
    "\n",
    "The dataset used in this project was presented in *\"Liputan6: A Large-scale Indonesian Dataset for Text Summarization\"* paper by Koto, et al. (2020). Following the paper, it contains 193.883 `train`, 10.972 `dev`, and 10.972 `test` document, with 16.1% of novel unigram, 52.5% novel bigram, 71.8% novel trigram, and 82.4% novel quadgram, with 311K vocabulary in article and 100K vocabulary in summary.\n",
    "\n",
    "It's worth to note that the writers provide two different datasets, namely Canonical and Extreme. The one I used here is the Canonical one, which is the heavier and \"original\" one with details explained above. I also need to add that the data were initially received with json format, which is versatile but memory-hungry, so I convert it to parquet in order to save time and computing resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>clean_article</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>extractive_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/100000/yudh...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Jakarta, :, Presiden, S...</td>\n",
       "      <td>[[Menurut, Presiden, Susilo, Bambang, Yudhoyon...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/100002/jepa...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Jakarta, :, Perdana, Me...</td>\n",
       "      <td>[[Pada, masa, silam, Jepang, terlalu, ambisius...</td>\n",
       "      <td>[2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/100003/pulu...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Kutai, :, Banjir, denga...</td>\n",
       "      <td>[[Puluhan, hektare, areal, persawahan, yang, s...</td>\n",
       "      <td>[1, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/100004/pres...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Jakarta, :, Presiden, S...</td>\n",
       "      <td>[[Sekjen, PBB, Kofi, Annan, memuji, langkah, P...</td>\n",
       "      <td>[2, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/100005/warg...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Solok, :, Warga, Kampun...</td>\n",
       "      <td>[[Untuk, mempercepat, pelaksanaan, belajar-men...</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193878</th>\n",
       "      <td>99995.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/99995/banji...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Kutai, :, Banjir, yang,...</td>\n",
       "      <td>[[Sebanyak, 25, kecamatan, di, Kutai, Barat, d...</td>\n",
       "      <td>[1, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193879</th>\n",
       "      <td>99996.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/99996/lima-...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Kabupaten, Gowa, :, Lim...</td>\n",
       "      <td>[[Ribuan, kubik, lumpur, dari, Gunung, Bawakar...</td>\n",
       "      <td>[3, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193880</th>\n",
       "      <td>99997.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/99997/kawas...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Nias, :, Sejumlah, desa...</td>\n",
       "      <td>[[Kawasan, paling, utara, di, Pulau, Nias, ,, ...</td>\n",
       "      <td>[1, 2, 6, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193881</th>\n",
       "      <td>99998.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/99998/kebak...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Bogor, :, Kebakaran, di...</td>\n",
       "      <td>[[Dari, bukti-bukti, di, lapangan, ,, kebakara...</td>\n",
       "      <td>[0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193882</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>https://www.liputan6.com/news/read/99999/rp-90...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Jakarta, :, Indikasi, k...</td>\n",
       "      <td>[[Hasil, audit, investigasi, BPK, terhadap, KP...</td>\n",
       "      <td>[0, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193883 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                                url  \\\n",
       "0       100000.0  https://www.liputan6.com/news/read/100000/yudh...   \n",
       "1       100002.0  https://www.liputan6.com/news/read/100002/jepa...   \n",
       "2       100003.0  https://www.liputan6.com/news/read/100003/pulu...   \n",
       "3       100004.0  https://www.liputan6.com/news/read/100004/pres...   \n",
       "4       100005.0  https://www.liputan6.com/news/read/100005/warg...   \n",
       "...          ...                                                ...   \n",
       "193878   99995.0  https://www.liputan6.com/news/read/99995/banji...   \n",
       "193879   99996.0  https://www.liputan6.com/news/read/99996/lima-...   \n",
       "193880   99997.0  https://www.liputan6.com/news/read/99997/kawas...   \n",
       "193881   99998.0  https://www.liputan6.com/news/read/99998/kebak...   \n",
       "193882   99999.0  https://www.liputan6.com/news/read/99999/rp-90...   \n",
       "\n",
       "                                            clean_article  \\\n",
       "0       [[Liputan6, ., com, ,, Jakarta, :, Presiden, S...   \n",
       "1       [[Liputan6, ., com, ,, Jakarta, :, Perdana, Me...   \n",
       "2       [[Liputan6, ., com, ,, Kutai, :, Banjir, denga...   \n",
       "3       [[Liputan6, ., com, ,, Jakarta, :, Presiden, S...   \n",
       "4       [[Liputan6, ., com, ,, Solok, :, Warga, Kampun...   \n",
       "...                                                   ...   \n",
       "193878  [[Liputan6, ., com, ,, Kutai, :, Banjir, yang,...   \n",
       "193879  [[Liputan6, ., com, ,, Kabupaten, Gowa, :, Lim...   \n",
       "193880  [[Liputan6, ., com, ,, Nias, :, Sejumlah, desa...   \n",
       "193881  [[Liputan6, ., com, ,, Bogor, :, Kebakaran, di...   \n",
       "193882  [[Liputan6, ., com, ,, Jakarta, :, Indikasi, k...   \n",
       "\n",
       "                                            clean_summary extractive_summary  \n",
       "0       [[Menurut, Presiden, Susilo, Bambang, Yudhoyon...             [0, 1]  \n",
       "1       [[Pada, masa, silam, Jepang, terlalu, ambisius...             [2, 3]  \n",
       "2       [[Puluhan, hektare, areal, persawahan, yang, s...             [1, 5]  \n",
       "3       [[Sekjen, PBB, Kofi, Annan, memuji, langkah, P...             [2, 5]  \n",
       "4       [[Untuk, mempercepat, pelaksanaan, belajar-men...             [0, 2]  \n",
       "...                                                   ...                ...  \n",
       "193878  [[Sebanyak, 25, kecamatan, di, Kutai, Barat, d...          [1, 4, 3]  \n",
       "193879  [[Ribuan, kubik, lumpur, dari, Gunung, Bawakar...             [3, 6]  \n",
       "193880  [[Kawasan, paling, utara, di, Pulau, Nias, ,, ...       [1, 2, 6, 3]  \n",
       "193881  [[Dari, bukti-bukti, di, lapangan, ,, kebakara...             [0, 3]  \n",
       "193882  [[Hasil, audit, investigasi, BPK, terhadap, KP...             [0, 3]  \n",
       "\n",
       "[193883 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_parquet():\n",
    "    #Testing lazy import! Might be handy for OOP...\n",
    "    import pandas as pd\n",
    "    import polars as pl\n",
    "\n",
    "    df = pl.read_parquet('train.parquet').to_pandas()\n",
    "    return df\n",
    "\n",
    "read_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting pandas to PyArrow with Lazy Import. Handy for saving memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193883 entries, 0 to 193882\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   id                  193883 non-null  float64\n",
      " 1   url                 193883 non-null  object \n",
      " 2   clean_article       193883 non-null  object \n",
      " 3   clean_summary       193883 non-null  object \n",
      " 4   extractive_summary  193883 non-null  object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 7.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_headline</th>\n",
       "      <th>clean_article</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>extractive_summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>yudhoyono berharap masalah kemiskinan menjadi ...</td>\n",
       "      <td>[[Liputan6, ., com, ,, Jakarta, :, Presiden, S...</td>\n",
       "      <td>[[Menurut, Presiden, Susilo, Bambang, Yudhoyon...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_headline  \\\n",
       "id                                                          \n",
       "100000  yudhoyono berharap masalah kemiskinan menjadi ...   \n",
       "\n",
       "                                            clean_article  \\\n",
       "id                                                          \n",
       "100000  [[Liputan6, ., com, ,, Jakarta, :, Presiden, S...   \n",
       "\n",
       "                                            clean_summary extractive_summary  \n",
       "id                                                                            \n",
       "100000  [[Menurut, Presiden, Susilo, Bambang, Yudhoyon...             [0, 1]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_headline'] = df['url'].apply(lambda x: x.split('/')[-1].replace('.html','').replace('-',' '))\n",
    "df['id'] = df['url'].apply(lambda x: x.split('/')[-2])\n",
    "df = df.drop(columns=['url'])\n",
    "df = df[['id','clean_headline','clean_article','clean_summary','extractive_summary']]\n",
    "df = df.set_index('id')\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.info` shown 0 null values in the dataframe, but `id` needlessly utilize `float64` data type. `url`, however, possess two dataparts which is usable in the analysis: First, the number part of the `url` can replace `id` as a string, and second, the part after it is actually the title of that article, which is usable as the article's headline. This creates a conditional probabilities as well: If a word is present in article, then it has a possibility to be included in the summary. Similarly, if that particular word is present in summary, it also has a probability to be included in the headline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is to clean stopwords using Louis Owen's Stoplist, with some additional words that I added. You can access this stoplist at https://github.com/louisowen6/NLP_bahasa_resources/blob/master/combined_stop_words.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the sheer size of this database creates a memory problem, and I did try my very best with both Kaggle and Google Colab to circumvent this. First, I tried to save each part (headline, article, and summary) as Sklearn's sparse matrix using `CountVectorizer`, but even the \"mere\" headline data exceeds both Kaggle's RAM (30gb) and Colab's Disk (100gb) size limit. Second, I did try to use an Indonesian HuggingFace model to tokenize these lists for EDA, but the clean_article hits the RAM limit and resets the kernel.\n",
    "\n",
    "The only way, then, is to use n-gram language model (which also means zero vectorization) with either lazy or parallel execution. Using `Pandas` single core and eager execution, combined with absolutely inadequate laptop specification, means hours and hours of compute time per cell (yes, I did try to create a vocabulary with all 3 lists using `stack().unique()`, `count()`, and `isin()` statement, and those statements finished in 298 minutes). I will need \"something-something\" optimize this task further, and my (current) solution is to use `joblib` to paralellize `pandas` apply statement.\n",
    "\n",
    "In case you're wondering why I didn't use `pandarallel` library, I did try to use it but encountered system memory problem. It seems that `pandarallel` has issues in accessing specific memory in Windows, but that's way beyond my knowledge for now.\n",
    "\n",
    "Let's start with removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    from joblib import Parallel, delayed #Lazy import and paralellization\n",
    "\n",
    "    with open ('combined_stop_words.txt', 'r') as f:\n",
    "        stop_words = f.read().splitlines()\n",
    "        stop_words = set(stop_words)\n",
    "        stopwordslist = [i for i in stop_words]\n",
    "\n",
    "    wordstoclean = ['liputan6', 'liputan', 'com', 'array', 'dtype', 'object', 'dtypeobject', 'sctv']\n",
    "\n",
    "    stopwordslist.extend(wordstoclean)\n",
    "\n",
    "    df = Parallel(n_jobs=-1)(delayed(lambda x: ' '.join([word for word in x.split() if word not in stopwordslist]))(text) for text in df)\n",
    "\n",
    "    return df\n",
    "\n",
    "#Astype(str) is necessary because the df is, somehow, a numpy.ndarray.\n",
    "df['clean_article'] = clean_text(df['clean_article'].astype(str))\n",
    "df['clean_summary'] = clean_text(df['clean_summary'].astype(str))\n",
    "df['extractive_summary'] = clean_text(df['extractive_summary'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to import pandas again after lazy importing!\n",
    "clean_headline = pd.read_pickle('clean_headline.pkl').to_list()\n",
    "clean_article = pd.read_pickle('clean_article.pkl').to_list()\n",
    "clean_summary = pd.read_pickle('clean_summary.pkl').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq\\\\backend\\\\cython\\\\checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m     ngram \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m'\u001b[39m:vocab, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigram\u001b[39m\u001b[38;5;124m'\u001b[39m:bigram, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrigram\u001b[39m\u001b[38;5;124m'\u001b[39m:trigram})\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ngram\n\u001b[1;32m---> 16\u001b[0m \u001b[43mngram_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_headline\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m, in \u001b[0;36mngram_analysis\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      9\u001b[0m bigram \u001b[38;5;241m=\u001b[39m FreqDist(ngrams(token, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     10\u001b[0m trigram \u001b[38;5;241m=\u001b[39m FreqDist(ngrams(token, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m ngram \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvocab\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mbigram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrigram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtrigram\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ngram\n",
      "File \u001b[1;32mc:\\Users\\reina\\Desktop\\Dev\\.venv\\Lib\\site-packages\\polars\\dataframe\\frame.py:367\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, schema, schema_overrides, strict, orient, infer_schema_length, nan_to_null)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m dict_to_pydf(\n\u001b[0;32m    363\u001b[0m         {}, schema\u001b[38;5;241m=\u001b[39mschema, schema_overrides\u001b[38;5;241m=\u001b[39mschema_overrides\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_pydf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, Sequence)):\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m sequence_to_pydf(\n\u001b[0;32m    377\u001b[0m         data,\n\u001b[0;32m    378\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m         infer_schema_length\u001b[38;5;241m=\u001b[39minfer_schema_length,\n\u001b[0;32m    383\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\reina\\Desktop\\Dev\\.venv\\Lib\\site-packages\\polars\\_utils\\construction\\dataframe.py:137\u001b[0m, in \u001b[0;36mdict_to_pydf\u001b[1;34m(data, schema, schema_overrides, strict, nan_to_null)\u001b[0m\n\u001b[0;32m    124\u001b[0m     data_series \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    125\u001b[0m         pl\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[0;32m    126\u001b[0m             name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m column_names\n\u001b[0;32m    133\u001b[0m     ]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     data_series \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    136\u001b[0m         s\u001b[38;5;241m.\u001b[39m_s\n\u001b[1;32m--> 137\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_expand_dict_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    143\u001b[0m     ]\n\u001b[0;32m    145\u001b[0m data_series \u001b[38;5;241m=\u001b[39m _handle_columns_arg(data_series, columns\u001b[38;5;241m=\u001b[39mcolumn_names, from_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    146\u001b[0m pydf \u001b[38;5;241m=\u001b[39m PyDataFrame(data_series)\n",
      "File \u001b[1;32mc:\\Users\\reina\\Desktop\\Dev\\.venv\\Lib\\site-packages\\polars\\_utils\\construction\\dataframe.py:388\u001b[0m, in \u001b[0;36m_expand_dict_values\u001b[1;34m(data, schema_overrides, strict, order, nan_to_null)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m((arrlen(val) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 388\u001b[0m             updated_data[name] \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_is_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m order \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(updated_data) \u001b[38;5;241m!=\u001b[39m order:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {col: updated_data\u001b[38;5;241m.\u001b[39mpop(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m order}\n",
      "File \u001b[1;32mc:\\Users\\reina\\Desktop\\Dev\\.venv\\Lib\\site-packages\\polars\\series\\series.py:311\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, name, values, dtype, strict, nan_to_null, dtype_if_empty)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, Sequence):\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s \u001b[38;5;241m=\u001b[39m \u001b[43msequence_to_pyseries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnan_to_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_to_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s \u001b[38;5;241m=\u001b[39m sequence_to_pyseries(name, [], dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\reina\\Desktop\\Dev\\.venv\\Lib\\site-packages\\polars\\_utils\\construction\\series.py:299\u001b[0m, in \u001b[0;36msequence_to_pyseries\u001b[1;34m(name, values, dtype, strict, nan_to_null)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m constructor \u001b[38;5;241m==\u001b[39m PySeries\u001b[38;5;241m.\u001b[39mnew_object:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m         srs \u001b[38;5;241m=\u001b[39m \u001b[43mPySeries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_from_any_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _check_for_numpy(python_dtype, check_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    301\u001b[0m             np\u001b[38;5;241m.\u001b[39mbool_(\u001b[38;5;28;01mTrue\u001b[39;00m), np\u001b[38;5;241m.\u001b[39mgeneric\n\u001b[0;32m    302\u001b[0m         ):\n\u001b[0;32m    303\u001b[0m             dtype \u001b[38;5;241m=\u001b[39m numpy_char_code_to_dtype(np\u001b[38;5;241m.\u001b[39mdtype(python_dtype)\u001b[38;5;241m.\u001b[39mchar)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "def ngram_analysis(x):\n",
    "    import nltk\n",
    "    from nltk.util import ngrams\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    token = word_tokenize(' '.join(x))\n",
    "    vocab = FreqDist(ngrams(token, 1))\n",
    "    bigram = FreqDist(ngrams(token, 2))\n",
    "    trigram = FreqDist(ngrams(token, 3))\n",
    "\n",
    "    ngram = pl.DataFrame({'vocab':vocab, 'bigram':bigram, 'trigram':trigram}).to_pandas()\n",
    "\n",
    "    return ngram\n",
    "\n",
    "ngram_analysis(clean_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
